= Connector Guides
:description: Hazelcast comes with a wide range of built-in connectors for accessing data in other platforms. You can also use Hazelcast with pre-built connectors for importing data from external systems. Both types of connector are configured to read and write data in the most efficient way for their respective platform.
:page-aliases: sql:connectors.adoc, pipelines:sources-sinks.adoc, integrate:redis-connector.adoc, integrate:twitter-connector.adoc

{description}

Built-in connectors are available as either sources or sinks. Sources allow Hazelcast to read data from the required platform either for xref:query:sql-overview.adoc[queries] or as part of a larger xref:pipelines:overview.adoc[data pipeline]. Sinks allow Hazelcast to write data, such as query results, to a target system.

You can also use pre-built link:https://www.confluent.io/hub/[Kafka Connect Source connectors] with Hazelcast. These low-code connectors allow you to stream events from non-Kafka external systems into a data pipeline ready for stream processing. Kafka Connect Source connectors are available for many popular platforms, including RabbitMQ, Neo4j, Couchbase, Scylla, SAP and Redis.

Explore the available connectors and choose one for your use case.

== Processing Guarantees

To ensure that all data is processed correctly, connectors that support streaming have either an xref:pipelines:configuring-jobs.adoc#setting-a-processing-guarantee-for-streaming-jobs[at-least-once or exactly-once processing guarantee].

Connectors that do not support streaming do not have any processing guarantees. Instead, during intermittent failures, you must restart any queries or jobs that use these connectors.

== Connectors for SQL

You can use the following connectors in SQL by configuring them in the xref:sql:create-mapping.adoc[`CREATE MAPPING` statement].

=== Sources

[cols="m,a,a,a"]
|===
|Connector|Module|Batch/Streaming|Processing guarantee


|xref:sql:mapping-to-kafka.adoc[Apache Kafka]
|hazelcast-jet-kafka
|streaming
|exactly-once

|xref:sql:mapping-to-a-file-system.adoc[File]
|Depends on the file format and/or storage system.
|Depends on the storage system.
|Depends on the storage system.

|xref:sql:mapping-to-maps.adoc[IMap]
|hazelcast-sql
|batch
|N/A

|xref:sql:mapping-to-jdbc.adoc[JDBC]
|hazelcast-sql
|batch
|N/A

|xref:sql:mapping-to-mongo.adoc[MongoDB]
|hazelcast-sql and hazelcast-jet-mongodb
|batch, streaming
|exactly-once

|===

=== Sinks

[cols="m,a,a,a"]
|===
|Connector|Module|Batch/Streaming|Processing guarantee

|xref:sql:mapping-to-kafka.adoc[Apache Kafka]
|hazelcast-jet-kafka
|streaming
|exactly-once

|xref:sql:mapping-to-maps.adoc[IMap]
|hazelcast-sql
|batch
|N/A

|xref:sql:mapping-to-jdbc.adoc[JDBC]
|hazelcast-sql
|batch
|N/A

|xref:sql:mapping-to-mongo.adoc[MongoDB]
|hazelcast-sql and hazelcast-jet-mongodb
|batch
|exactly-once

|===

== Kafka Connect Connectors

=== Source

Integrate the xref:integrate:kafka-connect-connectors.adoc[Kafka Connect Source] connector for your external system with Hazelcast by calling the `KafkaConnectSources.connect()` method in a pipeline.


[cols="m,m,a,a"]
|===
|Connector|Module|Batch/Streaming|Processing Guarantee

|xref:integrate:kafka-connect-connectors.adoc[KafkaConnectSources.connect]
|hazelcast-jet-kafka-connect
|streaming
|at-least-once

|===

== Connectors for the Jet API

The Jet API supports more connectors than SQL.

=== Sources

[cols="m,m,a,a"]
|===
|Connector|Module|Batch/Streaming|Processing guarantee

|xref:integrate:legacy-file-connector.adoc[AvroSources.files]
|hazelcast-jet-avro
|batch
|N/A

|xref:integrate:legacy-cdc-connectors.adoc[DebeziumCdcSources.debezium] (Legacy)
|hazelcast-jet-cdc-debezium
|streaming
|at-least-once

|xref:integrate:legacy-cdc-connectors.adoc[MySqlCdcSources.mysql] (Legacy)
|hazelcast-jet-cdc-mysql
|streaming
|exactly-once

|xref:integrate:legacy-cdc-connectors.adoc[PostgresCdcSources.postgres] (Legacy)
|hazelcast-jet-cdc-postgres
|streaming
|exactly-once

|xref:integrate:cdc-connectors.adoc[DebeziumCdcSources.debezium] ([.enterprise]*Enterprise*)
|hazelcast-enterprise-cdc-debezium
|streaming
|at-least-once

|xref:integrate:cdc-connectors.adoc[MySqlCdcSources.mysql]
|hazelcast-enterprise-cdc-mysql
|streaming
|exactly-once

|xref:integrate:cdc-connectors.adoc[PostgresCdcSources.postgres]
|hazelcast-enterprise-cdc-postgres
|streaming
|exactly-once

|xref:integrate:elasticsearch-connector.adoc[ElasticSources.elastic]
|hazelcast-jet-elasticsearch-7
|batch
|N/A

|xref:integrate:legacy-file-connector.adoc[HadoopSources.inputFormat]
|hazelcast-jet-hadoop
|batch
|N/A

|xref:integrate:kafka-connector.adoc[KafkaSources.kafka]
|hazelcast-jet-kafka
|streaming
|exactly-once

|xref:integrate:kinesis-connector.adoc[KinesisSources.kinesis]
|hazelcast-jet-kinesis
|streaming
|exactly-once

|xref:integrate:mongodb-connector.adoc[MongoSources.batch]
|hazelcast-jet-mongodb
|batch
|exactly-once

|xref:integrate:mongodb-connector.adoc[MongoSources.stream]
|hazelcast-jet-mongodb
|streaming
|exactly-once

|xref:integrate:pulsar-connector.adoc[PulsarSources.pulsarConsumer]
|hazelcast-jet-contrib-pulsar
|streaming
|N/A

|xref:integrate:pulsar-connector.adoc[PulsarSources.pulsarReader]
|hazelcast-jet-contrib-pulsar
|streaming
|exactly-once

|xref:integrate:legacy-file-connector.adoc[S3Sources.s3]
|hazelcast-jet-s3
|batch
|N/A

|xref:integrate:jcache-connector.adoc[Sources.cache]
|hazelcast-jet
|batch
|N/A

|xref:integrate:jcache-connector.adoc[Sources.cacheJournal]
|hazelcast-jet
|streaming
|exactly-once

|xref:integrate:file-connector.adoc[Sources.files]
|hazelcast-jet
|batch
|N/A

|xref:integrate:legacy-file-connector.adoc[Sources.fileWatcher]
|hazelcast-jet
|streaming
|none

|xref:integrate:legacy-file-connector.adoc[Sources.json]
|hazelcast-jet
|batch
|N/A

|xref:integrate:legacy-file-connector.adoc[Sources.jsonWatcher]
|hazelcast-jet
|streaming
|none

|xref:integrate:jdbc-connector.adoc[Sources.jdbc]
|hazelcast-jet
|batch
|N/A

|xref:integrate:jms-connector.adoc[Sources.jmsQueue]
|hazelcast-jet
|streaming
|exactly-once

|xref:integrate:list-connector.adoc[Sources.list]
|hazelcast-jet
|batch
|N/A

|xref:integrate:map-connector.adoc[Sources.map]
|hazelcast-jet
|batch
|N/A

|xref:integrate:map-connector.adoc[Sources.mapJournal]
|hazelcast-jet
|streaming
|exactly-once

|xref:integrate:socket-connector.adoc[Sources.socket]
|hazelcast-jet
|streaming
|none

|xref:integrate:legacy-file-connector.adoc#fvecs-and-ivecs[VectorSources.fvecs]
[.enterprise]*{enterprise-product-name}*
|hazelcast-enterprise
|batch
|N/A

|xref:integrate:legacy-file-connector.adoc#fvecs-and-ivecs[VectorSources.ivecs]
[.enterprise]*{enterprise-product-name}*
|hazelcast-enterprise
|batch
|N/A

|xref:integrate:test-connectors.adoc[TestSources.items]
|hazelcast-jet
|batch
|N/A

|xref:integrate:test-connectors.adoc[TestSources.itemStream]
|hazelcast-jet
|streaming
|none
|===

=== Sinks

[cols="m,m,a,a"]
|===
|Connector|Module|Batch/Streaming|Processing Guarantee

|xref:integrate:legacy-file-connector.adoc[AvroSinks.files]
|hazelcast-jet-avro
|batch
|N/A

|xref:integrate:cdc-connectors.adoc[CdcSinks.map]
|hazelcast-jet-cdc-debezium (legacy, {open-source-product-name})

or

hazelcast-enterprise-cdc-debezium ({enterprise-product-name})
|streaming
|at-least-once

|xref:integrate:elasticsearch-connector.adoc[ElasticSinks.elastic]
|hazelcast-jet-elasticsearch-7
|streaming
|at-least-once

|xref:integrate:legacy-file-connector.adoc[HadoopSinks.outputFormat]
|hazelcast-jet-hadoop
|batch
|N/A

|xref:integrate:kafka-connector.adoc[KafkaSinks.kafka]
|hazelcast-jet-kafka
|streaming
|exactly-once

|xref:integrate:kinesis-connector.adoc[KinesisSinks.kinesis]
|hazelcast-jet-kinesis
|streaming
|at-least-once

|xref:integrate:mongodb-connector.adoc[MongoSinks.mongodb]
|hazelcast-jet-kinesis
|batch, streaming
|exactly-once

|xref:integrate:pulsar-connector.adoc[PulsarSources.pulsarSink]
|hazelcast-jet-contrib-pulsar
|streaming
|at-least-once

|xref:integrate:legacy-file-connector.adoc[S3Sinks.s3]
|hazelcast-jet-s3
|batch
|N/A

|xref:integrate:jcache-connector.adoc[Sinks.cache]
|hazelcast-jet
|streaming
|at-least-once

|xref:integrate:legacy-file-connector.adoc[Sinks.files]
|hazelcast-jet
|streaming
|exactly-once

|xref:integrate:legacy-file-connector.adoc[Sinks.json]
|hazelcast-jet
|streaming
|exactly-once

|xref:integrate:jdbc-connector.adoc[Sinks.jdbc]
|hazelcast-jet
|streaming
|exactly-once

|xref:integrate:jms-connector.adoc[Sinks.jmsQueue]
|hazelcast-jet
|streaming
|exactly-once

|xref:integrate:list-connector.adoc[Sinks.list]
|hazelcast-jet
|batch
|N/A

|xref:integrate:map-connector.adoc[Sinks.map]
|hazelcast-jet
|streaming
|at-least-once

|xref:integrate:observable-connector.adoc[Sinks.observable]
|hazelcast-jet
|streaming
|at-least-once

|xref:integrate:reliable-topic-connector.adoc[Sinks.reliableTopic]
|hazelcast-jet
|streaming
|at-least-once

|xref:integrate:socket-connector.adoc[Sinks.socket]
|hazelcast-jet
|streaming
|at-least-once

|xref:integrate:vector-collection-connector.adoc[VectorSinks.vectorCollection]
[.enterprise]*{enterprise-product-name}*
|hazelcast-enterprise
|batch, streaming
|at-least-once

|===
